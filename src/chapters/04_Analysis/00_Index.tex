\chapter{Analysis}
This chapter presents an in-depth analysis of the performance metrics collected during
experiments involving containerization and functional package management systems. The goal
of this analysis is to compare key metrics such as build times and package sizes, which are
critical in evaluating the efficiency of these approaches for creating reproducible
environments and deploying software. The experiments were designed to be highly reproducible
and transparent, with all tests automated via continuous integration (CI) workflows. By
automating the process, consistent conditions were maintained throughout, allowing for
reliable comparisons between container-based environments and functional package management
systems. The results for these tests are publicly accessible on the designated GitHub repository
at \href{https://github.com/clemenscodes/webserver/actions}{GitHub}.

\section{Build Times}
Build times represent the total duration from the start of a build process to its completion,
making it a critical metric for evaluating the overall efficiency of development workflows,
especially in CI/CD (Continuous Integration/Continuous Deployment) environments. Reducing
build time can directly improve feedback cycles and accelerate the deployment of software
applications.

For containerized systems, build times are typically captured during the CI pipeline by
monitoring the build process of the container image. The build artifact contains information
such as layer creation timestamps and dependency resolution times. The CI workflow initiates
a fresh container build without any caching mechanisms in place to simulate a cold build.
This ensures that every part of the build process, from pulling dependencies to creating
image layers, is fully captured. Once the build process is complete, the container artifact
is logged, and the overall build time is measured by calculating the timestamps for each
layer creation and the final containerization step. This approach simulates real-world
conditions, where developers may need to build the application from scratch or with varying
degrees of caching and reuse.

For functional package management systems, build times are similarly captured during the CI
workflow. These systems operate by fetching dependencies defined in a declarative build
specification, building them in a highly reproducible and isolated environment. The build
duration is measured from the beginning of the build step, including the time spent resolving
dependencies, compiling source code, and creating the final package. The use of declarative
dependency trees ensures that the build process can be executed consistently, regardless of
the system on which it runs. The total build time is calculated based on the start and
successful completion of the CI build step, providing an accurate measure of the time
required to prepare the software for deployment.

Both containerization and functional package management systems were evaluated under three
distinct scenarios: cold builds, warm builds, and fully cached builds. A cold build refers to
a scenario where no prior caching is available, meaning that all dependencies and components
must be resolved and built from scratch. Warm builds involve the reuse of cached dependencies
and build artifacts, where some parts of the build process are invalidated due to source code
changes, but many components can still take advantage of previously cached results. In
contrast, fully cached builds represent a situation where no changes have been introduced to
the codebase, allowing the build process to completely skip any steps that can be reused
from the cache. These three scenarios offer insights into how effectively each system
manages caching, build optimization, and dependency resolution.

The following table summarizes the build times for both approaches across these scenarios:

\begin{table}[H]
	\centering
	\caption{Build Time Comparison between Containerization and Functional Package Management (in seconds)}
	\label{tab:build_times}
	\begin{tabular}{|l|c|c|c|}
		\hline
		\textbf{Build Type} & \textbf{Containerization} & \textbf{Functional Package Management} \\
		\hline
		Cold Build          & 244                       & 144                                    \\
		Warm Build          & 44                        & 144                                    \\
		Cached Build        & 9                         & 3                                      \\
		\hline
	\end{tabular}
\end{table}

Build time is a critical metric for evaluating the efficiency of containerization and
functional package management systems, especially in how they handle cold, warm, and fully
cached builds. Table \ref{tab:build_times} shows the distinct performance differences
between these approaches, particularly in their ability to optimize for different types of
builds and caching strategies.

In cold builds, functional package management systems tend to outperform containerization
due to their more modular and lightweight handling of dependencies. Container-based systems
often rely on downloading and assembling base images, which can introduce substantial
overhead, especially if the base images include an entire operating system. Downloading,
unpacking, and configuring these base images adds significant time to the overall build
process. In contrast, functional package managers are able to pull individual build-time
dependencies directly from predefined caches or, if necessary, build these dependencies
from source in a fully reproducible manner. This flexibility and avoidance of full image
assembly allow functional package management systems to complete cold builds faster. This
performance advantage is particularly noticeable in projects with numerous external
dependencies that can be retrieved from existing binary caches, reducing the need for
costly recompilation.

However, in warm builds, functional package management systems show only marginal
performance improvements over cold builds. This is because functional package managers
operate in isolated and immutable build environments, ensuring that each build remains
reproducible and unaffected by previous states. While this immutability is a core feature
of functional package management, guaranteeing consistent builds, it also limits the reuse
of intermediate build states. Even when some components of the build are cached, the strict
isolation enforced during the build process prevents significant reuse of these cached
outputs. As a result, the warm build times in functional package management remain similar
to those of cold builds, with minimal reductions in build time despite some caching.

Containerization systems, on the other hand, excel in warm builds due to their flexible
layer-based caching mechanisms. In these systems, the build process can be divided into
multiple stages, with each stage cached separately. This enables developers to optimize
warm builds by reusing previously built layers that are unaffected by recent changes. For
instance, in a scenario involving a Rust project, optimizations could be made by replacing
the main source file with a dummy file in an early stage of the build, allowing all
dependencies and external libraries to be cached. Later, the actual source code can be
inserted in a separate build layer, ensuring that only the code needs recompiling, while
the cached dependencies are reused. This level of build optimization is not possible in
functional package management due to the immutable nature of its build processes. As such,
containerized systems see significant improvements in warm builds, where only parts of the
build are recalculated, dramatically reducing build times.

When it comes to fully cached builds, both containerized systems and functional package
management approaches perform exceptionally well, with functional package managers often
having a slight edge in speed. In fully cached builds, no new changes are introduced,
allowing both systems to reuse previous build results completely. Functional package
managers are able to retrieve cached derivations with minimal overhead, especially when
using services such as binary caches to store pre-built artifacts. This results in extremely
fast build times. Containerization systems, while also benefiting from layer-based caching,
tend to involve slightly more overhead in managing layers and retrieving cached images,
although the difference in performance is minimal. Both systems are designed to avoid
redundant work when the code has not changed, ensuring rapid builds.

In summary, the build time results show the distinct strengths and weaknesses of these
approaches under different scenarios. Functional package management systems excel in cold
builds due to their modular handling of dependencies, but their immutability limits
performance gains in warm builds. In contrast, containerized systems, while slower in cold
builds due to the overhead of handling full images, significantly outperform in warm builds
through flexible caching strategies. Fully cached builds for both systems are optimized to
provide minimal overhead, showcasing the efficiency of well-designed caching mechanisms when
no changes are made to the codebase.

\section{Package Sizes}
The size of the final deployment artifact is a crucial factor when evaluating the efficiency
and practicality of different software packaging and deployment strategies. Smaller package
sizes generally result in faster installations, reduced storage consumption, and lower
bandwidth requirements. These factors become especially significant in environments where
resources such as disk space, network bandwidth, and deployment time are constrained.

In containerized environments, the size of the deployment artifact, typically represented by
a container image, is determined by the total size of the built image after it has been
assembled. This image generally includes the operating system, application code, and any
required dependencies. During the continuous integration (CI) process, container images are
built by layering the base operating system image with the application and its dependencies.
Once built, the final image is exported and analyzed, reflecting the total disk space
consumed by the image, which includes everything required to run the application in isolation.
This method ensures that the image size accurately reflects the actual deployment artifact,
which must include not only the application itself but also any system-level dependencies
and libraries required by the container's environment.

In contrast, functional package management systems produce much smaller deployment artifacts.
These systems build software in isolated, reproducible environments, ensuring that the
resulting package includes only the minimal runtime components necessary for execution. The
build process explicitly separates build-time dependencies from runtime dependencies, with
the former being discarded after the build completes. Functional package management systems
often leverage binary caches, which store pre-built artifacts containing only the essential
runtime dependencies. By avoiding the inclusion of extraneous system layers, functional
package management systems are able to create leaner, more efficient deployment artifacts.

Table \ref{tab:package_size} provides a comparison of package sizes between these two
approaches:

\begin{table}[H]
	\centering
	\caption{Package Size Comparison between Containerization and Functional Package Management (in MB)}
	\label{tab:package_size}
	\begin{tabular}{|l|c|c|}
		\hline
		\textbf{Artifact Type} & \textbf{Containerized Image} & \textbf{Functional PM Derivation} \\
		\hline
		Size                   & 31.82                        & 2.78                              \\
		\hline
	\end{tabular}
\end{table}

As shown in Table \ref{tab:package_size}, artifacts produced by functional package management
systems are significantly smaller than those generated by containerized systems. This size
difference is largely due to the architectural nature of the two approaches. In a containerized
system, the deployment artifact must include the entire operating system, even when using
minimalist base images such as those derived from lightweight distributions like Alpine Linux.
Although these base images are optimized to reduce unnecessary components, the fundamental
overhead of bundling the operating system and its libraries remains. This overhead is
characteristic of containerization, where each container operates as a self-contained
environment that isolates the application from the host system.

In contrast, functional package management systems aim to minimize redundancy by focusing
only on the essential components required to run the application. Since functional package
management systems isolate the build process and handle dependencies in a modular manner,
only the strict runtime dependencies are included in the final package. Moreover, once the
package has been built, any unnecessary build-time dependencies are discarded or can be
garbage collected, further reducing the overall size of the deployment artifact. This approach
results in significantly smaller packages because the inclusion of an entire operating system
or unused libraries is unnecessary in the functional package management paradigm.

The reduced size of packages generated by functional package management systems not only
translates to more space-efficient deployment but also contributes to faster installation
times. Smaller packages require less bandwidth during transfer, an important consideration
in environments where network resources are limited or costly. Additionally, the smaller
artifact size makes functional package management systems particularly advantageous when
applications need to be deployed across multiple servers or nodes, as faster deployment times
help minimize downtime and streamline the continuous integration and deployment (CI/CD)
processes. This is particularly critical in modern software development environments, where
efficiency and minimal resource consumption are key to maintaining competitive and responsive
workflows.

In summary, while containerized systems attempt to minimize package size by using lightweight
base images and optimizing dependency layers, they are inherently limited by the need to
bundle the operating system and all supporting libraries. Functional package management
systems, by contrast, avoid this overhead by focusing exclusively on runtime dependencies,
resulting in significantly smaller and more efficient deployment artifacts. This difference
in size not only improves installation speed but also enhances overall system efficiency,
particularly in bandwidth-limited or resource-constrained environments.

\section{Development Environments}
Both containerization technologies and functional package management systems are widely used
to create isolated, reproducible development environments, simplifying the management of
dependencies, tools, and workflows. However, these two approaches differ significantly in
how they handle environment configuration and integration with different operating systems.

In containerized environments, the development setup is encapsulated within containers that
include not only the application but also the operating system, necessary dependencies, and
tools. This ensures that the environment inside the container mirrors production environments
as closely as possible, providing a consistent and isolated space for development. Developers
can create an image that contains all required tools and dependencies for the project.
However, accessing this environment typically involves running the container, which is
completely isolated from the host system. This means that any custom configurations, shell
aliases, or developer-specific setups on the host are not automatically applied within the
container. Developers need to manually configure the development environment inside the
container, which can involve additional steps, such as connecting to the container and
setting up tools from scratch.

A particular challenge arises when using containerization on non-Linux platforms, such as
Windows, where containers rely on the Linux kernel for many operations. To enable
containerization, a virtual machine (VM) running a Linux kernel must be deployed in the
background. This introduces overhead, as developers must manage both the container itself
and the underlying VM. This can lead to higher resource consumption and may negatively impact
performance, especially when multiple containers are running simultaneously. Additionally,
file synchronization between the host system and the container may be slower, particularly
when the host cannot automatically notify the container of file changes. This often results
in manual container refreshes, which slow down the development iteration process and introduce
friction into the workflow.

In contrast, functional package management systems offer a more declarative approach to
managing development environments. Developers define their environment using a declarative
file (such as \texttt{shell.nix} in Nix), listing the exact dependencies and tools required
for the project. Instead of relying on containers or virtual machines, functional package
management systems set up the development environment directly in the developer’s existing
shell session. This allows developers to use their personal shell configurations, including
aliases and custom profiles, while seamlessly integrating the project-specific environment
into their workflow. There is no need to isolate the environment from the host system, and
the setup can be done without introducing significant overhead.

One of the key advantages of functional package management systems is their ability to pull
pre-built packages from binary caches. For example, developers can access fully functional
development environments without the need to build dependencies from source, as these
dependencies can be retrieved from pre-configured binary caches. Companies and teams can
host and manage their own binary caches, ensuring that developers can quickly install
dependencies, even if they are new to the project. This drastically reduces the time and
effort required to set up a new development environment, as pre-built environments can be
provided on a per-branch or per-project basis. For example, when switching between branches
in a version control system, the corresponding development environment for that branch can
be automatically loaded, ensuring that the appropriate dependencies and tools are available
without additional configuration steps. This feature is particularly useful for complex
projects where different branches might require varying toolsets or dependencies at different
stages of the project lifecycle.

\section{Process Management}
As projects grow in complexity, managing multiple services becomes essential, especially
when dealing with databases, message brokers, and other auxiliary services. Containerization
systems typically handle this through orchestration tools such as Docker Compose, which
allows developers to define and manage multiple containers simultaneously. Each service
can be run in its own isolated container, simulating a production-like environment while
keeping processes separate from one another. This approach is effective for isolating services
and managing complex workflows, though it adds the overhead of managing multiple containers,
each requiring its own resources.

Functional package management systems offer a different method for orchestrating multiple
services without relying on containers. For example, in systems like Nix, tools like
\textit{services-flake} provide a declarative approach to managing services directly on the
host system. Rather than running each service in a container, \textit{services-flake} enables
developers to define and manage services natively. Built on top of existing process management
tools, this system allows multiple services to run concurrently, each isolated from the others
without the overhead of virtual machines or containers. \textit{Services-flake} integrates
directly with project configuration files (such as \texttt{flake.nix}), ensuring that service
management is as reproducible and declarative as the package management process itself.

This approach offers a lightweight and platform-agnostic solution that works across macOS,
Linux, and other environments that support functional package management systems. The services
are run directly on the host system, with data stored in project-specific directories. In
addition, \textit{services-flake} offers a terminal-based interface for monitoring and managing
services, much like Docker Compose provides control over container-based services. This
native approach reduces the complexity associated with containers and avoids the additional
resource overhead, while still allowing developers to work in an isolated and reproducible
development environment.

By utilizing native service orchestration tools, developers can build and manage complex
development environments with multiple interdependent services, such as databases, caches,
and application components, without needing the extra layers of container technology. This
approach reduces complexity and enhances efficiency, particularly in development environments
where managing many services concurrently is required.

In contrast to containerization, which emphasizes process isolation through the use of
containerized environments, functional package management systems focus primarily on package
isolation. However, by leveraging tools such as \textit{services-flake}, these systems can
also achieve process isolation in a lightweight and efficient manner, enabling developers to
manage services and environments without the need for virtual machines or containerized
workflows. The declarative configuration language used in functional package management
systems ensures that these environments remain reproducible, easy to manage, and scalable
to the project’s needs.

\section{Software Deployments}
This section discusses the observed results from evaluating containerization and functional
package management systems in the context of software deployment. The analysis focuses on
aspects such as scalability, serverless capabilities, and infrastructure requirements,
highlighting how these two approaches manage the deployment and scaling of software applications.

Containerization offers a method for deploying software by packaging applications along
with their dependencies and the operating system into container images. These images include
the entire runtime environment, ensuring that the application runs consistently across various
platforms. The containerization approach abstracts the underlying system, allowing
applications to be deployed on any platform that supports container runtimes. This portability
makes containers suitable for both cloud-based and on-premise deployments, where maintaining
consistent environments across different systems is important.

In environments with fluctuating traffic, containerization works effectively with orchestration
tools such as Kubernetes. These tools automate the management and scaling of containers,
handling resource allocation based on demand. In serverless contexts, containers are capable
of dynamic scaling, with orchestration platforms spinning up new containers when traffic
increases and shutting them down when demand subsides. Kubernetes manages container lifecycle
and distribution, enabling horizontal scaling to meet variable traffic conditions. The observed
deployment results suggest that containerized systems, by bundling the entire environment,
offer a high degree of portability. Additionally, orchestration tools enable containerized
systems to quickly respond to changing resource needs, minimizing downtime and optimizing
resource utilization.

Functional package management systems take a different approach to deployment by focusing on
reproducibility. Instead of bundling the operating system with the application, these systems
ensure that each deployed package is reproducible, with dependencies strictly defined and isolated.
The aim is to deploy the exact same software environment on any system where the package manager
is installed. This model emphasizes a declarative build process, where packages are installed
and executed within target environments that have the package manager available. The key strength
of this approach lies in its ability to ensure consistent deployments across different systems
through precise dependency management.

However, scaling software deployed via functional package management tends to require more
manual intervention compared to the automated scaling seen in containerized environments.
To scale applications, additional servers that support the package manager must be provisioned,
or infrastructure automation tools need to be configured to handle server provisioning and
package installation. The observed results indicate that, while functional package management
systems guarantee reproducibility, the process of scaling these deployments is generally more
static and manual. Custom automation workflows may be needed to enable flexible scaling,
which contrasts with the built-in automation provided by container orchestration platforms.

Regarding scalability, the results suggest that containerization, particularly when combined
with orchestration tools like Kubernetes, demonstrates strong capabilities. Containers can be
deployed and scaled rapidly, making them well-suited for environments where traffic fluctuates
or where resources need to be dynamically allocated. Orchestration tools automate the scaling
process by monitoring resource usage and adjusting the number of containers accordingly. This
level of automation is particularly beneficial in cloud environments, where workloads often
experience unpredictable traffic patterns. By comparison, functional package management systems
appear to adopt a more static approach to scaling. To scale an application deployed via functional
package management, provisioning additional servers is required, with less emphasis on automated
resource allocation. The absence of built-in, dynamic scaling features presents a contrast to
the container-based approach, where scaling is handled in real-time by orchestration tools.

The analysis also highlights differences in process management. Functional package management
systems excel at maintaining reproducibility, ensuring that every instance of the software runs
in a consistent and isolated environment. However, this reproducibility comes with a trade-off
in scalability automation. For instance, scaling functional packages typically requires
deploying and configuring new servers manually, ensuring they are compatible with the package
manager. This contrasts with containerized deployments, where orchestration tools like Kubernetes
can dynamically adjust the number of running containers based on demand, enabling a more seamless
scaling process without requiring manual intervention.

While both approaches effectively deploy software in isolated environments, they differ in their
scaling mechanisms. The containerization model, with its orchestration integration, provides
rapid horizontal scaling. Containers can be deployed and terminated quickly, allowing applications
to scale based on demand. By abstracting the application from the underlying infrastructure,
containerized deployments are well-suited for environments where traffic fluctuates frequently
or unpredictably. In contrast, functional package management systems emphasize environment
reproducibility and consistency, focusing on package isolation rather than on dynamic process
scaling. As a result, scaling requires a more hands-on approach, involving infrastructure
provisioning and monitoring.

The observations from these deployment tests provide insights into the strengths and limitations
of both systems. Containerization appears to offer significant advantages in terms of scalability
and automation through orchestration, while functional package management systems offer
a high degree of reproducibility but may require more manual effort to achieve similar scaling
outcomes. These results serve as the basis for further discussion in the next chapter, where
conclusions will be drawn based on the analysis presented here.
